{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTE-2501 Week 2 Lecture Notes: \n",
    "\n",
    "## Basic Terminology in Supervised Learning\n",
    "\n",
    "### Supervised Learning\n",
    "- **Definition**: Trains a model on known input-output pairs to make future predictions.\n",
    "- **Example**: Training a spam filter using emails labeled as spam or not spam.\n",
    "\n",
    "### Notations\n",
    "- **$ùëã$**: Set of inputs\n",
    "- **$ùëå$**: Set of outputs\n",
    "- **$ùë¶: ùëã ‚Üí ùëå$**: Target function, the ideal function we aim to approximate.\n",
    "- **$ùëé: ùëã ‚Üí ùëå$**: Algorithm or decision function, our model's approximation of $ùë¶$.\n",
    "- **$ùëô$**: Cardinality of the data set, i.e., the number of samples in the training data.\n",
    "- **$ùë¶ùëñ = ùë¶(ùë•ùëñ)$**: Known responses or outputs for training samples.\n",
    "\n",
    "### Features\n",
    "- **Feature (attribute)**: A characteristic used to describe an input, denoted as a mapping $ùëì: ùëã ‚Üí ùê∑ùëì$.\n",
    "- **Feature Description**: A vector of features for a specific input.\n",
    "- **Feature Data Matrix**: A table containing feature descriptions for all inputs in the training data.\n",
    "\n",
    "$$ \n",
    "F = \\left( f_j(x_i) \\right)_{l \\times n} = \n",
    "\\begin{pmatrix}\n",
    "f_1(x_1) & \\cdots & f_n(x_1) \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "f_1(x_l) & \\cdots & f_n(x_l)\n",
    "\\end{pmatrix}\n",
    "$$ \n",
    "\n",
    "# Main Differences Between Unsupervised and Supervised Learning\n",
    "\n",
    "### Supervision\n",
    "- **Supervised Learning**: Utilizes labeled data for training.\n",
    "- **Unsupervised Learning**: Utilizes unlabeled data for training.\n",
    "\n",
    "### Objective\n",
    "- **Supervised Learning**: Aims to learn a mapping from inputs to outputs.\n",
    "- **Unsupervised Learning**: Aims to find hidden patterns or structures in the data.\n",
    "\n",
    "## Methods Used\n",
    "\n",
    "### Methods for Supervised Learning\n",
    "1. **Linear Regression**\n",
    "2. **Logistic Regression**\n",
    "3. **Decision Trees**\n",
    "4. **Random Forests**\n",
    "5. **Support Vector Machines**\n",
    "6. **Neural Networks**\n",
    "\n",
    "### Methods for Unsupervised Learning\n",
    "1. **Clustering** (e.g., K-means, Hierarchical clustering)\n",
    "2. **Dimensionality Reduction** (e.g., PCA, t-SNE)\n",
    "3. **Anomaly Detection** (e.g., Isolation Forest)\n",
    "4. **Association Rule Mining** (e.g., Apriori)\n",
    "5. **Autoencoders**\n",
    "\n",
    "\n",
    "# Feature types\n",
    "\n",
    "#### Quantitative\n",
    "1. **Discrete**: Numeric features that take on specific, often integer, values.  \n",
    "   *Example*: Number of rooms in a house.\n",
    "   \n",
    "2. **Continuous**: Numeric features that can take on any value within a range.  \n",
    "   *Example*: Temperature, height.\n",
    "\n",
    "#### Qualitative\n",
    "1. **Nominal**: Categorical features that do not have an intrinsic order.  \n",
    "   *Example*: Color, country names.\n",
    "   \n",
    "2. **Ordinal**: Categorical features that have a meaningful order, but intervals between values are not consistent.  \n",
    "   *Example*: Star ratings.\n",
    "\n",
    "3. **Binary**: Features that can take only two values.  \n",
    "   *Example*: True/False, 0/1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Methodology with Iris Dataset\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "### 1. Acquire the Relevant Dataset\n",
    "\n",
    "The Iris dataset is commonly used for machine learning experimentation and is readily available in many libraries like scikit-learn. Let's start by importing the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Convert to a DataFrame for easier manipulation\n",
    "df = pd.DataFrame(data=iris['data'], columns=iris['feature_names'])\n",
    "df['target'] = iris['target']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Identifying the Missing Values\n",
    "\n",
    "Although the Iris dataset is usually clean, in practice, datasets may contain missing values. We can identify these using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Splitting the Data Set into Two Separate Sets: Training Set and Test Set\n",
    "\n",
    "Splitting the dataset into training and test sets is crucial for assessing the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Features and Labels\n",
    "X = df.iloc[:, :-1]\n",
    "y = df['target']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Feature Scaling: Standardization and Normalization\n",
    "\n",
    "Feature scaling is an important step, especially for algorithms that rely on the magnitude of the features. We will look into two methods: standardization and normalization.\n",
    "\n",
    "#### Standardization\n",
    "\n",
    "The formula for standardization is:\n",
    "\n",
    "$$\n",
    "x' = \\frac{x - \\text{mean}(x)}{\\sigma}\n",
    "$$\n",
    "\n",
    "Here's how to perform standardization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training set and transform both training and test set\n",
    "X_train_standardized = scaler.fit_transform(X_train)\n",
    "X_test_standardized = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization\n",
    "\n",
    "The formula for normalization is:\n",
    "\n",
    "$$\n",
    "x' = \\frac{x - \\text{min}(x)}{\\text{max}(x) - \\text{min}(x)} $$\n",
    "\n",
    "Here's how to perform normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit on training set and transform both training and test set\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELI5: Standardization and Normalization\n",
    "\n",
    "#### What is Standardization?\n",
    "\n",
    "Imagine you have a basket of apples and oranges, and you are trying to compare their weights. But here's the catch: apples are weighed in grams, and oranges are weighed in pounds. It would be like comparing apples to oranges, literally! Standardization is like converting all the weights to a common scale where the average is zero, and the scale of the weights is the same.\n",
    "\n",
    "In math terms, you take each weight, subtract the average weight, and then divide by the standard deviation (a measure of how spread out the weights are).\n",
    "\n",
    "$$[\n",
    "x' = \\frac{x - \\text{mean}(x)}{\\sigma}\n",
    "$$]\n",
    "\n",
    "#### What is Normalization?\n",
    "\n",
    "Now, let's say you have a race between a snail and a rabbit. The snail covers a distance of 2 meters, while the rabbit covers 200 meters. Normalization is like converting their distances into percentages of the race they have completed.\n",
    "\n",
    "In math terms, you take each distance, subtract the smallest distance, and then divide by the range of distances (max distance - min distance).\n",
    "\n",
    "$$[\n",
    "x' = \\frac{x - \\text{min}(x)}{\\text{max}(x) - \\text{min}(x)}\n",
    "$$]\n",
    "\n",
    "#### Why Are They Important?\n",
    "\n",
    "Both standardization and normalization are methods to scale your features, so that no particular feature has undue influence on the model's performance. This is crucial for machine learning algorithms that use distance metrics (like k-NN) or gradient descent (like neural networks) because these algorithms are sensitive to the magnitude of the features. By transforming the features, you make it easier for the algorithm to learn from the data and often improve the model's performance.\n",
    "\n",
    "So, whether you're comparing apples to oranges, or racing snails against rabbits, standardization and normalization help put things on a level playing field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the style of seaborn for better visualization\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a pairplot to visualize the relationships between features\n",
    "sns.pairplot(df, hue='target', vars=['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELI5: What is Feature Generation?\n",
    "\n",
    "#### Simple Explanation\n",
    "\n",
    "Imagine you're trying to predict how well a student will do in a math test. You have their grades in English and Science, but you know that math involves both language skills and logical reasoning. Feature generation is like creating a new 'subject' that combines both English and Science to give you a better idea of how the student might perform in math.\n",
    "\n",
    "#### How It Works in Regression\n",
    "\n",
    "In a regression problem, you're trying to predict a number (like the math grade) based on other numbers (like English and Science grades). Feature generation is about creating new numbers (features) that could help you make a better prediction.\n",
    "\n",
    "---\n",
    "\n",
    "### ELI Somewhat More Advanced: What is Feature Generation?\n",
    "\n",
    "#### Detailed Explanation\n",
    "\n",
    "In machine learning, especially in regression problems, the quality of your features can often determine the quality of your model. Feature generation involves creating new features from the existing ones, or even bringing in entirely new data that could be relevant to the problem. For example, in predicting house prices, if you have the width and length of the house, you might generate a new feature called 'Area' by multiplying them.\n",
    "\n",
    "#### Mathematical Perspective\n",
    "\n",
    "Mathematically, feature generation can involve various operations such as:\n",
    "\n",
    "- **Linear Combinations**: $( NewFeature = a \\times Feature1 + b \\times Feature2 )$\n",
    "- **Polynomial Features**: $( NewFeature = Feature1^{2}, Feature1 \\times Feature2 )$\n",
    "- **Logarithmic or Exponential Transformations**: $( NewFeature = \\log(Feature1) )$\n",
    "- **Interaction Terms**: $( NewFeature = Feature1 \\times Feature2 \\times Feature3 )$\n",
    "\n",
    "#### Importance in Regression\n",
    "\n",
    "In regression, feature generation can provide the model with more information, capture hidden relationships in the data, and often improve the model's performance. However, it's crucial to be cautious, as adding too many features can lead to overfitting, where the model becomes too complex and performs poorly on new data.\n",
    "\n",
    "So, feature generation is like giving your model extra tools to solve the problem, but you have to make sure it doesn't get overwhelmed with too many tools.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function in Machine Learning\n",
    "\n",
    "## Overview\n",
    "\n",
    "In machine learning, the primary goal is to find an optimal algorithm that performs well on a given dataset. To quantify how well an algorithm performs, we use a loss function \\(\\epsilon(a, x)\\). The loss function measures the discrepancy between the predicted output $(a(x) )$ and the true output $(y(x) )$ for a training sample $(x)$.\n",
    "\n",
    "The loss function $(\\epsilon(a, x))$ depends on the type of problem we are trying to solve:\n",
    "\n",
    "## Types of Loss Functions\n",
    "\n",
    "### Classification\n",
    "\n",
    "In classification problems, the loss function is usually an error indicator. It is a boolean variable that is 1 if the algorithm misclassifies a sample and 0 otherwise.\n",
    "\n",
    "$$\n",
    "\\epsilon(a, x) = [a(x) \\neq y(x)]\n",
    "$$\n",
    "\n",
    "### Regression\n",
    "\n",
    "In regression problems, common loss functions include:\n",
    "\n",
    "1. **Absolute Error**: Measures the absolute difference between the predicted and true outputs.\n",
    "\n",
    "$$\n",
    "\\epsilon(a, x) = |a(x) - y(x)|\n",
    "$$\n",
    "\n",
    "2. **Squared Error**: Measures the squared difference between the predicted and true outputs.\n",
    "\n",
    "$$\n",
    "\\epsilon(a, x) = (a(x) - y(x))^2\n",
    "$$\n",
    "\n",
    "## Empirical Risk\n",
    "\n",
    "To evaluate the performance of an algorithm on the entire dataset, we introduce the concept of *empirical risk*. It is essentially the average loss over all training samples.\n",
    "\n",
    "$$\n",
    "Q(a, X_l) = \\frac{1}{l} \\sum_{i=1}^{l} \\epsilon(a, x_i)\n",
    "$$\n",
    "\n",
    "Here, $(X_l)$ is the training dataset containing $(l)$ samples. The objective in machine learning is often to minimize this empirical risk to find the best algorithm $(a)$.\n",
    "\n",
    "In summary, the loss function serves as the cornerstone in optimizing machine learning algorithms. Different types of problems require different types of loss functions, and the empirical risk gives us a single value that summarizes the algorithm's performance on the training dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELI5: Minimization of the Empirical Risk \n",
    "\n",
    "Think of empirical risk minimization like trying to get the best grades in your class. You have various subjects (features), like Math, Science, and English. Your grade (the output) depends on how well you perform in each of these subjects. \n",
    "\n",
    "Imagine you're trying different study methods to improve your grades. The \"best method\" is the one that gives you the highest grades while putting in the least amount of study time.\n",
    "\n",
    "In the given example, think of:\n",
    "- $X_l$ as different combinations of time you spend studying different subjects.\n",
    "- $a$ as a study method you're trying out.\n",
    "- $Q(a, X_l)$ as how good or bad your grades are with a given study method and time combination.\n",
    "- $arg \\min$ means you're looking for the method and time combination that gives you the best grades with least effort.\n",
    "\n",
    "So, $\\mu(X_l) = arg \\min_a Q(a, X_l)$ means you're trying to find the best study method that gives you the best grades for the time you put in for each subject.\n",
    "\n",
    "---\n",
    "\n",
    "# ELI More Advanced: Empirical Risk Minimization\n",
    "\n",
    "Empirical Risk Minimization (ERM) is a principle used in machine learning for finding a model that best fits the available data. Here, $\\mu$ represents a learning method that takes a dataset $X_l$ and finds the model parameters that minimize some loss function $Q(a, X_l)$.\n",
    "\n",
    "- $\\mu$: Learning method\n",
    "- $X_l$: Dataset\n",
    "- $Q(a, X_l)$: The \"risk\" or error of using model $a$ on dataset $X_l$\n",
    "- $arg \\min_a Q(a, X_l)$: Find the model $a$ that minimizes this risk\n",
    "\n",
    "## Linear Regression as an Example\n",
    "\n",
    "In the case of linear regression, we have:\n",
    "- $Y = \\mathbb{R}$: Real-valued outputs\n",
    "- $n$ features $f_j: X \\to \\mathbb{R}$, for $j = 1, \\dots, n$\n",
    "- Linear model: $g(x_i, \\theta) = \\sum_{j=1}^{n} \\theta_j f_j(x)$, with $\\theta \\in \\mathbb{R}^n$\n",
    "- Squared error: $\\epsilon(a, x) = (a(x) - y(x))^2$\n",
    "\n",
    "For linear regression, ERM becomes a least squares optimization problem:\n",
    "\n",
    "$$[\n",
    "\\mu(X_l) = arg \\min_\\theta \\sum_{i=1}^{l} (g(x_i, \\theta) - y_i)^2\n",
    "]$$\n",
    "\n",
    "This finds the values of $\\theta$ that minimize the sum of the squared differences between the predicted and actual outputs, effectively \"fitting\" the model to the data.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dte-2501",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
